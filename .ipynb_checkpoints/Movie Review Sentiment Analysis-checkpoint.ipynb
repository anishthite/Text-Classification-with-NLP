{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the necessary imports\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is an example of a movie review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./aclImdb/train/pos/10327_7.txt', encoding=\"utf8\")\n",
    "print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting the train and test data into a pandas dataframe. It shuffles the data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                    text  sentiment\n",
       " 0      Totally ridiculous. If you know anything about...          0\n",
       " 1      My Take: A tired formula Christmas comedy. The...          0\n",
       " 2      This Lifetime style movie takes the middle age...          0\n",
       " 3      The writers and producers of this little outin...          0\n",
       " 4      This movie is not that interesting, except for...          0\n",
       " 5      Bear in mind, any film (let alone documentary)...          1\n",
       " 6      Alice(Claire Danes) and Darlene(Kate Beckinsal...          1\n",
       " 7      This movie has a slew of great adult stars but...          0\n",
       " 8      If the themes of The Girl From Missouri sound ...          1\n",
       " 9      Jon Voight is brilliant in Midnight Cowboy, bu...          1\n",
       " 10     To borrow from Dorothy Parker: This is not a f...          0\n",
       " 11     I thought that this film was very well made, H...          1\n",
       " 12     Director Ron Atkins is certifiably insane. Thi...          0\n",
       " 13     This film provides us with an interesting remi...          1\n",
       " 14     Mr. Destiny - 3.5/5 Stars<br /><br />\"Mr. Dest...          1\n",
       " 15     One of the best movies I ever saw was an Irish...          1\n",
       " 16     It must be the most corniest TV show on the ai...          0\n",
       " 17     I saw the description of the movie on TCM and ...          1\n",
       " 18     It is one of the better Indian movies I have s...          1\n",
       " 19     Sadly, 8 Simple Rules, for dating my teenage d...          1\n",
       " 20     ******SPOILERS******<br /><br />The unfunny ra...          0\n",
       " 21     I really liked this movie because I have a hus...          1\n",
       " 22     REVOLT OF THE ZOMBIES (2 outta 5 stars) No, th...          0\n",
       " 23     I wasn't alive in the 60's, so I can't guarant...          1\n",
       " 24     The larger-than-life figures of Wyatt Earp and...          1\n",
       " 25     I anticipated the release of the film as much ...          0\n",
       " 26     WWE has produced some of the worst pay-per-vie...          1\n",
       " 27     Just got out and cannot believe what a brillia...          1\n",
       " 28     I'm a big fan of films where people get conned...          1\n",
       " 29     The plot of 'House of Games' is the strongest ...          0\n",
       " ...                                                  ...        ...\n",
       " 24970  This movie is not only the funniest film ever ...          1\n",
       " 24971  As shallow as it may sound, I actually delayed...          1\n",
       " 24972  This game is fun and it has a plot that you co...          1\n",
       " 24973  Every time I watch this movie I am more impres...          1\n",
       " 24974  A long time ago, I watched this movie from the...          1\n",
       " 24975  This type of plot really does have a lot of po...          0\n",
       " 24976  Saw this my last day at the festival, and was ...          1\n",
       " 24977  This miserable film is a remake of a 1927 film...          0\n",
       " 24978  The script was VERY weak w/o enough character ...          0\n",
       " 24979  I saw this movie a long time ago... luckily it...          0\n",
       " 24980  Watching this stinker constitutes cruel and un...          0\n",
       " 24981  Take 4 couples whose relationships were alread...          1\n",
       " 24982  Did not know what to expect from from Van Damm...          1\n",
       " 24983  I thought it was a New-York located movie: wro...          0\n",
       " 24984  Guy Richie's third proper film (not counting t...          0\n",
       " 24985  While I would say I enjoy the show, I expected...          1\n",
       " 24986  Vampires, sexy guys, guns and some blood. Who ...          1\n",
       " 24987  I didn't think it could be done, but something...          0\n",
       " 24988  This movie had me going. The title was perhaps...          0\n",
       " 24989  This is one of those awful, sex-driven B-movie...          0\n",
       " 24990  This is an excellent documentary, packed with ...          1\n",
       " 24991  GBS wrote his own screen adaptation of this No...          1\n",
       " 24992  Listen, I don't care what anybody says, as Cyp...          1\n",
       " 24993  This film is terrible. The story concerns a wo...          0\n",
       " 24994  \"Fever Pitch\" isn't a bad film; it's a terribl...          0\n",
       " 24995  Una giornata particolare is a film which has m...          1\n",
       " 24996  This is Jackie Chan's best film, and my person...          1\n",
       " 24997  I had somewhat high hopes for this since I lik...          0\n",
       " 24998  Spoiler!! I love Branagh, love Helena Bonham-C...          0\n",
       " 24999  This was the worst movie I have ever seen and ...          0\n",
       " \n",
       " [25000 rows x 2 columns],\n",
       "                                                     text  sentiment\n",
       " 0      Even though this film was nothing special as s...          0\n",
       " 1      How do I describe the horrors?!!! First, some ...          0\n",
       " 2      I was pleasantly surprised to find a very enjo...          1\n",
       " 3      In one of the best of Charlie Chaplin's length...          1\n",
       " 4      Not many television shows appeal to quite as m...          1\n",
       " 5      Where do we start with an offering like this? ...          0\n",
       " 6      ...But it definitely still only deserves 4/10 ...          0\n",
       " 7      Its a sin how these things are made, but then ...          0\n",
       " 8      Half of the movie is is flashing lights and sh...          0\n",
       " 9      I am not sure who recommended Surveillance to ...          0\n",
       " 10     I knew I was in for a LONG 90 minutes when the...          0\n",
       " 11     I'm afraid I must disagree with Mr. Radcliffe,...          1\n",
       " 12     When I first watched this film, I thought that...          0\n",
       " 13     Yahoo Serious is like a $3 bottle of wine - ha...          0\n",
       " 14     OMG, it is seriously the best show in the worl...          1\n",
       " 15     I saw this movie last night and thought it was...          1\n",
       " 16     Don't get me wrong this was fun to watch. It h...          1\n",
       " 17     A lot of people give this movie a lot of crap,...          1\n",
       " 18     I have to ask myself, do movies like this get ...          0\n",
       " 19     Crossfire remains one of the best Hollywood me...          1\n",
       " 20     It must be so difficult to tell a story where ...          1\n",
       " 21     I know that Full Moon, or any other film studi...          0\n",
       " 22     This is a classic B type movie that you'd rath...          0\n",
       " 23     Well, after seeing \"Beginning\" I thought why t...          0\n",
       " 24     I saw this today with little background on wha...          1\n",
       " 25     I'm amazed that \"The Hospital\" has been so wel...          0\n",
       " 26     There's lots of ketchup but not a whole lot of...          0\n",
       " 27     If it is true that the movie only cost 150K to...          0\n",
       " 28     This is a terrible movie, and I'm not even sur...          0\n",
       " 29     The most generic, surface-level biography you ...          0\n",
       " ...                                                  ...        ...\n",
       " 24970  Anthony Wong plays Lok,a husband whose wife is...          1\n",
       " 24971  This home movie is basically scandalously rubb...          0\n",
       " 24972  One of the latest (disaster) movies from York ...          0\n",
       " 24973  This comic classic of English school girl anti...          1\n",
       " 24974  Very Cliched. Quite corny. Acting gets worse a...          0\n",
       " 24975  One of the more obscure of Anthony Mann's West...          1\n",
       " 24976  Firstly, this movie works in the fact that it ...          0\n",
       " 24977  Little did I know that when I signed up the th...          0\n",
       " 24978  Lucio Fulci, a director not exactly renowned f...          0\n",
       " 24979  one of the best and most inspirational movies ...          1\n",
       " 24980  Good footage of World War I-era ships and plan...          1\n",
       " 24981  I thought it was an excellent movie. Gary Cole...          1\n",
       " 24982  Oh man , this movie is amazing, It's very good...          1\n",
       " 24983  First of all, I really can't understand how so...          0\n",
       " 24984  Cheaply pieced together of recycled film foota...          0\n",
       " 24985  I couldn't believe the eye candy from start to...          1\n",
       " 24986  This movie was, unfortunately, terrible.<br />...          0\n",
       " 24987  The only conceivable flaw of this film is it's...          1\n",
       " 24988  I got this thing off the sci-fi shelf because ...          0\n",
       " 24989  Enchanted April was one of Harry Beaumont's la...          0\n",
       " 24990  This movie is called \"Solomon Kane\". Which it ...          0\n",
       " 24991  The idea that anyone could of concocted such a...          0\n",
       " 24992  We see at the beginning of Little Dieter Needs...          1\n",
       " 24993  I found this to be a watchable all be it very ...          0\n",
       " 24994  B. Kennedy tried to make a sequel by exaggerat...          0\n",
       " 24995  Here's a spoof that's guaranteed to entertain ...          0\n",
       " 24996  PLOT SPOILERS!!!! Dr. Boch (George C. Scott) i...          1\n",
       " 24997  There is no way to put into words just how bad...          0\n",
       " 24998  \"Nagisa no Shindobaddo\" or \"Like Grains of San...          1\n",
       " 24999  Now, I loved \"Lethal Weapon\" and \"Kiss Kiss Ba...          0\n",
       " \n",
       " [25000 rows x 2 columns])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = \"./aclImdb\" #Make sure you put the data folder in the same directory as this jupyter notebook file\n",
    "labeledData = {}\n",
    "for i in [\"train\", \"test\"]:\n",
    "    labeledData[i] = []\n",
    "    for sentiment in [\"pos\", \"neg\"]:\n",
    "        score = 1 if sentiment == \"pos\" else 0\n",
    "        path = os.path.join(directory, i, sentiment)\n",
    "        for filename in os.listdir(path):\n",
    "            with open(os.path.join(path, filename), encoding=\"utf8\") as f:\n",
    "                labeledData[i].append([f.read(), score])  #Initially adds them to separate lists\n",
    "\n",
    "np.random.shuffle(labeledData[\"train\"]) #Shuffling\n",
    "labeledData[\"train\"] = pd.DataFrame(labeledData[\"train\"], columns = ['text', 'sentiment']) #Putting them in a dataframe\n",
    "np.random.shuffle(labeledData[\"test\"])\n",
    "labeledData[\"test\"] = pd.DataFrame(labeledData[\"test\"], columns = ['text', 'sentiment'])\n",
    "labeledData[\"train\"], labeledData[\"test\"] #Prints out both pandas dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = labeledData[\"train\"].to_csv(r'C:\\Users\\mdodda3-gtri\\Untitled Folder\\train.csv.gz', index = None, header=False, compression='gzip')\n",
    "test = labeledData[\"test\"].to_csv(r'C:\\Users\\mdodda3-gtri\\Untitled Folder\\test.csv.gz', index = None, header=False, compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train1 = labeledData[\"train\"].iloc[:6250].to_csv (r'C:\\Users\\mdodda3-gtri\\Untitled Folder\\train1.csv.gz', index = None, header=False, compression='gzip')\n",
    "# train2 = labeledData[\"train\"].iloc[6250:12500].to_csv (r'C:\\Users\\mdodda3-gtri\\Untitled Folder\\train2.csv.gz', index = None, header=False, compression='gzip')\n",
    "# train3 = labeledData[\"train\"].iloc[12500:18750].to_csv (r'C:\\Users\\mdodda3-gtri\\Untitled Folder\\train3.csv.gz', index = None, header=False, compression='gzip')\n",
    "# train4 = labeledData[\"train\"].iloc[18750:].to_csv (r'C:\\Users\\mdodda3-gtri\\Untitled Folder\\train4.csv.gz', index = None, header=False, compression='gzip')\n",
    "\n",
    "# test1 = labeledData[\"test\"].iloc[:6250].to_csv (r'C:\\Users\\mdodda3-gtri\\Untitled Folder\\test1.csv.gz', index = None, header=False, compression='gzip')\n",
    "# test2 = labeledData[\"test\"].iloc[6250:12500].to_csv (r'C:\\Users\\mdodda3-gtri\\Untitled Folder\\test2.csv.gz', index = None, header=False, compression='gzip')\n",
    "# test3 = labeledData[\"test\"].iloc[12500:18750].to_csv (r'C:\\Users\\mdodda3-gtri\\Untitled Folder\\test3.csv.gz', index = None, header=False, compression='gzip')\n",
    "# test4 = labeledData[\"test\"].iloc[18750:].to_csv (r'C:\\Users\\mdodda3-gtri\\Untitled Folder\\test4.csv.gz', index = None, header=False, compression='gzip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column contains the movie reviews in separated rows.\n",
    "The second column indicates whether the review is a positive or negative review. \n",
    "A positive reivew has 7-10 stars, A negative review has 1-4 stars. 5-6 stars are disregarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import  word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "stemmer =SnowballStemmer(\"english\")\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the movie reviews into Specific Bag of Words Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn imports\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit, LogisticRegression, SGDClassifier, PassiveAggressiveClassifier, Perceptron, RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, ComplementNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.feature_extraction import stop_words as stopwords\n",
    "\n",
    "\n",
    "\n",
    "#Stop word list created to be used\n",
    "stop_words = ['in', 'of', 'at', 'a', 'the']\n",
    "\n",
    "#how each review will be vectorized\n",
    "vectorizer = CountVectorizer(stop_words=stop_words, #These stop words are removed\n",
    "                             binary=True, # if it contains in list it's 1, else it is 0\n",
    "                             ngram_range=(1,2), #contains pairs of words as well\n",
    "                            \n",
    "                            )\n",
    "newVect = CountVectorizer (tokenizer = LemmaTokenizer(),\n",
    "                           stop_words=stop_words,\n",
    "                           binary=True,\n",
    "                           ngram_range=(1,2))\n",
    "\n",
    "\n",
    "\n",
    "x_train = vectorizer.fit_transform(labeledData[\"train\"][\"text\"]) #reviews from train dataframe are vectorized\n",
    "x_test = vectorizer.transform(labeledData[\"test\"][\"text\"])  #reviews from test dataframe are vectorized\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.text import Tokenizer, one_hot, text_to_word_sequence\n",
    "# import keras\n",
    "# import gensim\n",
    "# train_data = labeledData[\"train\"][\"text\"]\n",
    "# test_data = labeledData[\"test\"][\"text\"]\n",
    "\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(train_data)\n",
    "# #x_train = tokenizer.texts_to_sequences(train_data)\n",
    "# #x_test = tokenizer.texts_to_sequences(test_data)\n",
    "# type(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model, training, and applying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of model: 0.89516\n"
     ]
    }
   ],
   "source": [
    "#Any linear model can be used\n",
    "model = LinearSVC(C=1)  \n",
    "#This is another model that can be used\n",
    "model2 = LogisticRegression()   \n",
    "model4 = SGDClassifier()\n",
    "model5 = PassiveAggressiveClassifier()\n",
    "model6 = NuSVC()\n",
    "model6 = NearestCentroid()\n",
    "#model trained associating review vectors to it sentiment scores \n",
    "model.fit(x_train, labeledData[\"train\"][\"sentiment\"]) \n",
    "#applying model on test data creates what model thinks is sentiment scores associated with each movie review\n",
    "y_pred = model.predict(x_test) \n",
    "#accuracy score created by comparing to actual sentiment score to model's predicted sentiment score\n",
    "acc = accuracy_score(labeledData[\"test\"][\"sentiment\"], y_pred)\n",
    "print(\"Accuracy score of model: \"+str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11154,  1346],\n",
       "       [ 1275, 11225]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix as cm\n",
    "cm(labeledData[\"test\"][\"sentiment\"], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15668: about\n",
      "17100: about game\n",
      "17501: about it\n",
      "18302: about poker\n",
      "19127: about things\n",
      "19556: above\n",
      "19845: above think\n",
      "20469: absolutely\n",
      "20496: absolutely appalling\n",
      "28893: actor\n"
     ]
    }
   ],
   "source": [
    "#This prints out the first 10 n-grams associated with the first review above and the correspoding indeces for each n-gram\n",
    "count = 10;\n",
    "for i in x_train[0].nonzero()[1]:\n",
    "    print(str(i)+\": \" + vectorizer.get_feature_names()[i])\n",
    "    count-=1\n",
    "    if (count == 0):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This a word2vec model created by Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "#gzip.open(\"C:\\Users\\mdodda3-gtri\\emade\\datasets\\movie_reviews\\GoogleNews-vectors-negative300.bin.gz\")\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a word2vec model and making each movie review the sum of all the vectors associated with each word in the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(labeledData[\"train\"][\"text\"])\n",
    "# x_train = tokenizer.texts_to_sequences(labeledData[\"train\"][\"text\"])\n",
    "# x_test =  tokenizer.texts_to_sequences(labeledData[\"train\"][\"text\"])\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "stop_words = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "stop_words = ['in', 'of', 'at', 'a', 'the']\n",
    "\n",
    "def tokenize(val):\n",
    "    ans = text_to_word_sequence(val)\n",
    "    ans = [a for a in ans if not a in stop_words]\n",
    "    return ans\n",
    "reviews = labeledData[\"train\"][\"text\"].append(labeledData[\"test\"][\"text\"], ignore_index=True).values\n",
    "\n",
    "words = [tokenize(val) for val in reviews.tolist()]\n",
    "#make a word2vec model using movie review dataset\n",
    "model = gensim.models.Word2Vec(sentences=words, size=100, window = 1000, workers = 4, min_count=1, sg = 0)\n",
    "#associate each word to a word vector and making the mo\n",
    "def method(list, wv):\n",
    "    mean = []\n",
    "    for word in list:\n",
    "        if word in wv.vocab:\n",
    "            mean.append(wv[word])\n",
    "        else:\n",
    "            a.append(word)\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = np.array([method(review, wv) for review in reviews.tolist()])\n",
    "#x_train.shape\n",
    "x_train = words[:25000]\n",
    "x_train = np.array([method(review, model.wv) for review in x_train])\n",
    "x_test = words[25000:]\n",
    "x_test = np.array([method(review, model.wv) for review in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()\n",
    "\n",
    "model.fit(x_train, labeledData[\"train\"][\"sentiment\"]) \n",
    "#applying model on test data creates what model thinks is sentiment scores associated with each movie review\n",
    "y_pred = model.predict(x_test) \n",
    "#accuracy score created by comparing to actual sentiment score to model's predicted sentiment score\n",
    "acc = accuracy_score(labeledData[\"test\"][\"sentiment\"], y_pred)\n",
    "print(\"Accuracy score of model: \"+str(acc))\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# turn a movie review string into a sequence of words \n",
    "import gensim\n",
    "def tokenize(val):\n",
    "    ans = text_to_word_sequence(val)\n",
    "    ans = [a for a in ans if not a in stop_words]\n",
    "    return ans\n",
    "\n",
    "x_train_list = [tokenize(val) for val in labeledData[\"train\"][\"text\"].values.tolist()]\n",
    "x_test_list = [tokenize(val) for val in labeledData[\"test\"][\"text\"].values.tolist()]\n",
    "reviews = labeledData[\"train\"][\"text\"].append(labeledData[\"test\"][\"text\"], ignore_index=True).values\n",
    "words = [tokenize(val) for val in reviews.tolist()]\n",
    "a =[]\n",
    "model = gensim.models.Word2Vec(sentences=words, size=100, window = 8, workers = 4, min_count=1, sg = 0)\n",
    "def method(list, wv):\n",
    "    mean = []\n",
    "    for word in list:\n",
    "        if word in wv.vocab:\n",
    "            mean.append(wv[word])\n",
    "        else:\n",
    "            a.append(word)\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "x_train = np.array([method(review, model) for review in x_train_list])\n",
    "x_test = np.array([method(review, model) for review in x_test_list])\n",
    "\n",
    "\n",
    "model = LinearSVC()\n",
    "\n",
    "model.fit(x_train, labeledData[\"train\"][\"sentiment\"]) \n",
    "#applying model on test data creates what model thinks is sentiment scores associated with each movie review\n",
    "y_pred = model.predict(x_test) \n",
    "#accuracy score created by comparing to actual sentiment score to model's predicted sentiment score\n",
    "acc = accuracy_score(labeledData[\"test\"][\"sentiment\"], y_pred)\n",
    "print(\"Accuracy score of model: \"+str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # nltk.download('punkt')\n",
    "# # [nltk.word_tokenize(sentences) for sentences in train]\n",
    "# # model = gensim.models.Word2Vec(sentences = )\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# wv.wv.most_similar('horrible')\n",
    "\n",
    "\n",
    "\n",
    "# tokenizer_obj = Tokenizer()\n",
    "# tokenizer_obj.fit_on_texts(labeledData[\"train\"][\"text\"])\n",
    "# sequences = tokenizer_obj.texts_to_sequences(labeledData[\"train\"][\"text\"])\n",
    "# word_index = tokenizer_obj.word_index\n",
    "# max_length = max([len(s.split()) for s in labeledData[\"train\"][\"text\"] + labeledData[\"test\"][\"text\"]])\n",
    "# review_pad = pad_sequences(sequences, maxlen = max_length)\n",
    "\n",
    "# num_words = len(word_index) + 1\n",
    "# embedding_matrix = np.zeros((num_words, 100))\n",
    "\n",
    "# for word, i in word_index.items():\n",
    "#     if i > num_words:\n",
    "#         continue\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "# type(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier, AdaBoostClassifier,ExtraTreesClassifier\n",
    "\n",
    "\n",
    "#pipe = Pipeline([('vect', CountVectorizer(binary=True, ngram_range=(1,2))),('clf',LogisticRegression(C=.05))])fg\n",
    "stop_words = ['in', 'of', 'at', 'a', 'the']\n",
    "\n",
    "##pipe = Pipeline([('vect', CountVectorizer(binary=True, ngram_range=(1,2), stop_words=stop_words)), ('clf',LinearSVC(C=.01))])##\n",
    "\n",
    "#Making pipeline. has vectorizer, uses tfidf, and uses multinomialnb for the model\n",
    "pipe1 = Pipeline([('vect', CountVectorizer(tokenizer = LemmaTokenizer(), binary=True, ngram_range=(1,2), stop_words=stop_words)), \n",
    "                 ('clf',SGDClassifier())])\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(stop_words=stop_words, binary=True, ngram_range=(1,2) )), ('clf', SGDClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fits whole pipeline using the train text as x value and train sentiment as y value and makes model\n",
    "for pipe in [pipe1, pipe2]:\n",
    "    pipe.fit(labeledData[\"train\"][\"text\"], labeledData[\"train\"][\"sentiment\"]) \n",
    "    #model predicts sentiment scores using test dataframe\n",
    "    y_pred = pipe.predict(labeledData[\"test\"][\"text\"])\n",
    "    #compares model sentiment scores to actual sentiment scores\n",
    "    acc = accuracy_score(labeledData[\"test\"][\"sentiment\"], y_pred)\n",
    "    print(\"Accuracy score of the model: \"+ str(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
