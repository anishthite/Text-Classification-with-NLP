{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the necessary imports\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                    text  sentiment\n",
       " 0      I first saw Martin's Day when I was just 10 ye...          1\n",
       " 1      There has been a political documentary, of rec...          1\n",
       " 2      First I liked that movie. It seemed to me a ni...          0\n",
       " 3      This is an intimate movie of a sincere girl in...          1\n",
       " 4      This is the first film of the Horrorfest I hav...          0\n",
       " 5      This is a German film from 1974 that is someth...          0\n",
       " 6      \"Mr. Harvey Lights a Candle\" is anchored by a ...          1\n",
       " 7      This film was on late at night when I saw it. ...          0\n",
       " 8      This film, along with WESTFRONT 1918, are my f...          1\n",
       " 9      Terrible use of scene cuts. All continuity is ...          0\n",
       " 10     I'm not saying that because the production val...          0\n",
       " 11     I'm sorry for Jean, after having such a good o...          0\n",
       " 12     Oh man, this s-u-c-k-e-d sucked.... I couldn't...          0\n",
       " 13     Van Damme. What else can I say? Bill Goldberg....          0\n",
       " 14     I wasn't expecting much of this film- a fun li...          0\n",
       " 15     A lot is dated in this episode (just like most...          1\n",
       " 16     One of my all time favourite films, ever. Just...          1\n",
       " 17     Greetings again from the darkness. What a reli...          1\n",
       " 18     Ming The Merciless does a little Bardwork and ...          0\n",
       " 19     I saw not so fabulous rating on IMDb, but I we...          0\n",
       " 20     A chemist develops a fabric that never gets di...          1\n",
       " 21     There is good. There is bad. And then their is...          0\n",
       " 22     I really loved this film, yes, I know it was f...          1\n",
       " 23     Beautifully done. A lot of angst. Friendship m...          1\n",
       " 24     I have seen about a thousand horror films. (my...          0\n",
       " 25     Leave it to geniuses like Ventura Pons, the Sp...          0\n",
       " 26     The 1930' were a golden age of Los Angeles wit...          0\n",
       " 27     For those who like their murder mysteries busy...          1\n",
       " 28     This movie is truly brilliant. It ducks throug...          0\n",
       " 29     This is one of the finest music concerts anyon...          1\n",
       " ...                                                  ...        ...\n",
       " 24970  Everything about this film is hog wash. Pitifu...          0\n",
       " 24971  I decided to watch this serial after seeing th...          0\n",
       " 24972  This movie is unbelievably ridiculous. I love ...          0\n",
       " 24973  What percentage of movies does a person go to ...          1\n",
       " 24974  OK first of all the video looks like it was fi...          0\n",
       " 24975  While the premise of the film is pretty lame (...          1\n",
       " 24976  My 2 year old likes the Doodlebops show, it se...          1\n",
       " 24977  A famous show master enters the elevator with ...          0\n",
       " 24978  Having heard so many people raving about this ...          0\n",
       " 24979  When I first saw this film in cinema 11 years ...          0\n",
       " 24980  Thanks for killing the franchise with this tur...          0\n",
       " 24981  To call this anything at all would be an insul...          0\n",
       " 24982  Hello everyone, This is my first time posting ...          1\n",
       " 24983  Oh my, this was the worst reunion movie I have...          0\n",
       " 24984  Lost is one of a kind...its so enchanting and ...          1\n",
       " 24985  To be honest, I had no idea what this movie wa...          1\n",
       " 24986  \"The Mother\" is a weird low-budget movie, touc...          1\n",
       " 24987  Awful dreams, wild premonitions, blasphemy and...          1\n",
       " 24988  Don't be fooled by the silly title folks, this...          1\n",
       " 24989  This is the best film the Derek couple has eve...          0\n",
       " 24990  Really started the 80s trend of disgusting vio...          0\n",
       " 24991  There was a Bugs Bunny cartoon titled \"Baby Bu...          0\n",
       " 24992  Sure, for it's super imagery and awesome sound...          1\n",
       " 24993  Plot: an amorous couple decide to engage in so...          0\n",
       " 24994  This is the best movie I've come across in a l...          1\n",
       " 24995  Saw this today with my 8 year old. I thought i...          1\n",
       " 24996  I saw this film a while ago on a Video CD.<br ...          0\n",
       " 24997  Not often have i had the feeling of a movie it...          1\n",
       " 24998  Witty and disgusting. Brash and intelligent. B...          1\n",
       " 24999  The acting is good, the women are beautiful, a...          0\n",
       " \n",
       " [25000 rows x 2 columns],\n",
       "                                                     text  sentiment\n",
       " 0      I was about 14 years old as I saw the musical ...          1\n",
       " 1      After seeing Meredith in \"Beyond the Prairie\" ...          1\n",
       " 2      Just see it! It's a smart movie but too hard t...          1\n",
       " 3      I was waiting for this movie for a time. In th...          0\n",
       " 4      My son Adam (5 years old) is a big Scooby Doo ...          1\n",
       " 5      Skip all the subjective \"this is a great film\"...          0\n",
       " 6      Strange enough, shorts like this get a 10. Why...          1\n",
       " 7      The story of Tom Garner opens with his grand f...          1\n",
       " 8      Now, I am going to do this without putting spo...          0\n",
       " 9      In this era when almost everything makes it on...          1\n",
       " 10     I was a guest at the Sept. 30th screening of E...          1\n",
       " 11     We all know what Chan-wook Park can do. If you...          1\n",
       " 12     When you're used to Lana Turner in the Postman...          0\n",
       " 13     This movie is very modern and forward. It is a...          0\n",
       " 14     I normally don't try and second guess a crime ...          0\n",
       " 15     If you want to waste a small portion of your l...          0\n",
       " 16     This movie tries to rip off Predator, but that...          0\n",
       " 17     This tatty am dram adaptation scrambles soules...          0\n",
       " 18     While I understood, that this show is too weir...          1\n",
       " 19     I've never watched a file in a language I don'...          1\n",
       " 20     I got this movie out a week after the death of...          1\n",
       " 21     This film had a couple of funny parts,but for ...          0\n",
       " 22     I absolutely adore this movie! I had never hea...          1\n",
       " 23     I must admit, I was against this movie from th...          0\n",
       " 24     From a perspective that it is possible to make...          1\n",
       " 25     First off, the initial concept of a lost fortu...          0\n",
       " 26     Perhaps I'm being too generous when I give thi...          0\n",
       " 27     I think that the costumes were excellent, and ...          1\n",
       " 28     I've never understood this type of spoof movie...          0\n",
       " 29     If you are looking for a movie that doesn't ta...          1\n",
       " ...                                                  ...        ...\n",
       " 24970  If you came here, it's because you've already ...          0\n",
       " 24971  I am finding that I get less and less excited ...          0\n",
       " 24972  This movie is just great... It starts out real...          1\n",
       " 24973  Its a very good comedy movie.Ijust liked it.I ...          1\n",
       " 24974  I don't think I've ever gave something a 1/10 ...          0\n",
       " 24975  (Warning: Some spoilers ahead.)<br /><br />Wha...          0\n",
       " 24976  Michael Ritchie's \"The Couch Trip\" is a wonder...          1\n",
       " 24977  A rather charming depiction of European union ...          1\n",
       " 24978  This movie was terrible. It is not something t...          0\n",
       " 24979  This was more of a love story than one about a...          0\n",
       " 24980  This is the first film I've watched from the I...          0\n",
       " 24981  OK, I read the director's comment about this m...          0\n",
       " 24982  I had been very curious to see the original si...          1\n",
       " 24983  Mike Nichols in finest form. I was not a fan o...          1\n",
       " 24984  Despite of all the negative criticism I really...          1\n",
       " 24985  It was so very long ago (1960), but I have nev...          1\n",
       " 24986  Not the film to see if you want to be intellec...          1\n",
       " 24987  Changi has a delightfully fresh script, acted ...          1\n",
       " 24988  Chaplin is a doughboy in his final film of 191...          1\n",
       " 24989  This is one of those \"family\" movies that I ca...          0\n",
       " 24990  A nice, humorous mix of music hall (in the fir...          1\n",
       " 24991  This is a pale imitation of 'Officer and a Gen...          0\n",
       " 24992  The success of the original French \"Emmanuelle...          0\n",
       " 24993  This movie dethroned Dr. Giggles as the best h...          1\n",
       " 24994  Like a Circle around the human condition, 2001...          1\n",
       " 24995  For those of us who are part of the real world...          0\n",
       " 24996  Why it's none other than Ator played hilarious...          0\n",
       " 24997  This film has the guts to suggest that it migh...          1\n",
       " 24998  Ok, so I saw this movie at this year's Sundanc...          0\n",
       " 24999  When it comes to movies, I don't easily discri...          1\n",
       " \n",
       " [25000 rows x 2 columns])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = \"./aclImdb\" #Make sure you put the data folder in the same directory as this jupyter notebook file\n",
    "labeledData = {}\n",
    "for i in [\"train\", \"test\"]:\n",
    "    labeledData[i] = []\n",
    "    for sentiment in [\"pos\", \"neg\"]:\n",
    "        score = 1 if sentiment == \"pos\" else 0\n",
    "        path = os.path.join(directory, i, sentiment)\n",
    "        for filename in os.listdir(path):\n",
    "            with open(os.path.join(path, filename), encoding=\"utf8\") as f:\n",
    "                labeledData[i].append([f.read(), score])  #Initially adds them to separate lists\n",
    "\n",
    "np.random.shuffle(labeledData[\"train\"]) #Shuffling\n",
    "labeledData[\"train\"] = pd.DataFrame(labeledData[\"train\"], columns = ['text', 'sentiment']) #Putting them in a dataframe\n",
    "np.random.shuffle(labeledData[\"test\"])\n",
    "labeledData[\"test\"] = pd.DataFrame(labeledData[\"test\"], columns = ['text', 'sentiment'])\n",
    "labeledData[\"train\"], labeledData[\"test\"] #Prints out both pandas dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/anishthite/miniconda3/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.24.2) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer, one_hot, text_to_word_sequence\n",
    "\n",
    "tokenizer = Tokenizer(num_words=6000)\n",
    "tokenizer.fit_on_texts(labeledData[\"train\"][\"text\"])\n",
    "x_train = tokenizer.texts_to_sequences(labeledData[\"train\"][\"text\"])\n",
    "x_test = tokenizer.texts_to_sequences(labeledData[\"test\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "max_words = 500\n",
    "x_train_pad = sequence.pad_sequences(x_train, maxlen=max_words)\n",
    "x_test_pad = sequence.pad_sequences(x_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 500)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index\n",
    "np.amax(x_train_pad) + 1\n",
    "x_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# from keras.preprocessing.text import Tokenizer, one_hot, text_to_word_sequence\n",
    "\n",
    "# import time\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# stop_words = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "# stop_words = ['in', 'of', 'at', 'a', 'the']\n",
    "# def tokenize(val):\n",
    "#     ans = text_to_word_sequence(val)\n",
    "#     ans = [a for a in ans if not a in stop_words]\n",
    "#     return ans\n",
    "# reviews = labeledData[\"train\"][\"text\"].append(labeledData[\"test\"][\"text\"], ignore_index=True).values\n",
    "\n",
    "# words = [tokenize(val) for val in reviews.tolist()]\n",
    "\n",
    "# model = gensim.models.Word2Vec(sentences=words, size=50, window = 100, workers = 8, min_count=1, sg = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 100)          600000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                17024     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 617,057\n",
      "Trainable params: 617,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, GRU, Bidirectional, GlobalMaxPool1D\n",
    "vocab_size = np.amax(x_train_pad) + 1\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_words))\n",
    "\n",
    "model.add(LSTM(32))\n",
    "\n",
    "# model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
    "# model.add(GlobalMaxPool1D())\n",
    "# model.add(Dense(20, activation=\"relu\"))\n",
    "#model.add(Dropout(0.05))\n",
    "\n",
    "#model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "print(vocab_size)\n",
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model2 = Sequential()\n",
    "#model2.add(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "13568/25000 [===============>..............] - ETA: 1:14 - loss: 0.5264 - acc: 0.7417"
     ]
    }
   ],
   "source": [
    "model.fit(x_train_pad, labeledData[\"train\"][\"sentiment\"].values, \n",
    "          batch_size = 128, epochs = 1, #validation_split=0.2\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test_pad, labeledData[\"test\"][\"sentiment\"], verbose=0)\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 epoch only\n",
    "#0.87552 - 128 batch size .2 validation_split using max value 52 seconds\n",
    "#0.8658 - 128 batch size .2 validation_split using max value 52 seconds\n",
    "#0.87056 145 s using 5000 32 batch size\n",
    "#0.85432 196s using max value 32 batch size\n",
    "#model.\n",
    "\n",
    "#default: 128 batch size 0 validation_splist 32 lstm size acc: .869\n",
    "# #default with dropout layer .874\n",
    "# print(scores)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = model.predict(x_test_pad)\n",
    "acc = accuracy_score(labeledData[\"test\"][\"sentiment\"], np.around(y_pred))\n",
    "print(\"Accuracy score of model: \"+str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.around(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding(vocab_size, 100, input_length=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = Sequential()\n",
    "model.layers[0].get_weights()[0].shape\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding([model.layers[0].get_weights()[0]], 100, weights = [model.layers[0].get_weights()[0]], input_length = max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def setup_pareto_front(pareto_info, method, func_num, summary_type, gen, trial, auc):\n",
    "# #     combo = sorted(pareto_info, key=lambda x: x[0])\n",
    "# #     pop_1 = [a for a, b in combo]\n",
    "# #     pop_2 = [b for a, b in combo]\n",
    "    \n",
    "#     plt.scatter(pop_1, pop_2, color='g')\n",
    "#     plt.plot(pop_1, pop_2, color='b', drawstyle='steps-post')\n",
    "# #     plt.xlabel(fitness_index_dict[0])\n",
    "# #     plt.ylabel(fitness_index_dict[1])\n",
    "#     plt.title(f\"Evolution Type: {method}\\nFunction: {func_num}\\nBest Pareto Front for gen{gen} trial{trial}\\n{summary_type}auc:{auc}\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_pareto_front([(327,342), (341,337.25), (342.25,339.5), (337.25, 348.25), (339.75,348.25), (333, 356), (309.75, 380.25), (356, 336.25), ()] , \"text classification\", \"coolness\", \"idk\", 34, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# with open('movie3.csv', newline='') as lines:\n",
    "#      spamreader = csv.reader(lines, quotechar='\"', delimiter=',',\n",
    "#                      quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "#      for row in spamreader:\n",
    "#         print(row[2:3])\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"movie3.csv\", header = None)\n",
    "df[df.columns[2:4]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(df[2], df[3], color='g')\n",
    "# plt.plot(pop_1, pop_2, color='b', drawstyle='steps-post')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dominates(a, b):\n",
    "    \"\"\"\n",
    "    Test if a dominates b\n",
    "    :param a:\n",
    "    :param b:\n",
    "    :return: boolean\n",
    "    \"\"\"\n",
    "    return np.all(a <= b)\n",
    "\n",
    "def my_pareto(data):\n",
    "    pareto_points = []\n",
    "    for i, x in enumerate(data):\n",
    "        pareto_status = True\n",
    "        pareto_points_to_remove = []\n",
    "        for j in pareto_points:\n",
    "            # If a pareto point dominates our point, we already know we are not pareto\n",
    "            if dominates(data[j], x):\n",
    "                pareto_status = False\n",
    "                break\n",
    "            # If our point dominates a Pareto point, that point is no longer Pareto\n",
    "            if dominates(x, data[j]):\n",
    "                pareto_points_to_remove.append(j)\n",
    "        # Remove elements by value\n",
    "        for non_pareto_point in pareto_points_to_remove:\n",
    "            pareto_points.remove(non_pareto_point)\n",
    "        if pareto_status:\n",
    "            pareto_points.append(i)\n",
    "    return pareto_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "list = my_pareto(df[df.columns[2:4]].values)\n",
    "df = df.iloc[list]\n",
    "df = df.sort_values(by=[2])\n",
    "df = df.iloc[9:30-7]\n",
    "pop1 = df[2]\n",
    "pop2 = df[3]\n",
    "plt.scatter(pop1, pop2, s = 20, color='g')\n",
    "plt.plot(pop1, pop2, color='b', drawstyle='steps-post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "st = [\"TfidfVectorizer with Single Learner MultiNB\", \n",
    "\"TfidfVectorizer with Single Learner MultiNB\", \n",
    "      \"TfidfVectorizer with Single Learner MultiNB\", \n",
    "       \"TfidfVectorizer with Single Learner MultiNB\", \n",
    "       \"CountVectorizer with Single Learner MultiNB\", \n",
    "       \"CountVectorizer with Single Learner MultiNB\", \n",
    "       \"TfidfVectorizer with Bagged Learner Passive\", \n",
    "\"TfidfVectorizer with Single Learner LinSVC\",\n",
    " \"TfidfVectorizer and Grid Search Learner SGD\", \n",
    " \"TfidfVectorizer and Single Learner Passive\",\n",
    " \"TfidfVectorizer and Bagged Learner Passive\", \n",
    " \"Tfidf and Single Learner Passive\", \n",
    " \"Tfidf and Single Learner Passive\", \n",
    "      \"Tfidf and Single Learner Passive\" ]\n",
    "\n",
    "\n",
    "colors = ['g','g', 'g', 'g', 'lime', 'lime', 'lightcoral', 'y', 'k', 'r', 'lightcoral', 'r', 'r', 'r']\n",
    "plt.title('Pareto Front for Optimal Individuals in IMDB Dataset')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"False Negative Rate\")\n",
    "falsePos = (df[2]/6249).values\n",
    "falseNeg = (df[3]/6249).values\n",
    "\n",
    "for i in range(len(df)):\n",
    "    stt = st[i]\n",
    "    pop1 = falsePos[i]\n",
    "    pop2 = falseNeg[i]\n",
    "    plt.scatter(pop1, pop2, s = 40, color=colors[i], label = colors[i],)\n",
    "    #plt.text(pop1+.05, pop2+.05, stt, fontsize=9)\n",
    "popp1 = falsePos\n",
    "popp2 = falseNeg\n",
    "plt.xlim([.030, .06])\n",
    "plt.plot(popp1, popp2, color='b', drawstyle='steps-post', linewidth = 1)\n",
    "\n",
    "falseNeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_elements = [\n",
    "                   Line2D([0], [0], marker='o', color='w', label='TfidfVectorizer with Single Learner MultiNB',\n",
    "                          markerfacecolor='g', markersize=8),\n",
    "     Line2D([0], [0], marker='o', color='w', label='CountVectorizer with Single Learner MultiNB',\n",
    "                          markerfacecolor='lime', markersize=8),\n",
    "     Line2D([0], [0], marker='o', color='w', label='TfidfVectorizer with Bagged Learner Passive',\n",
    "                          markerfacecolor='lightcoral', markersize=8),\n",
    "     Line2D([0], [0], marker='o', color='w', label='TfidfVectorizer with Single Learner LinSVC',\n",
    "                          markerfacecolor='y', markersize=8),\n",
    "     Line2D([0], [0], marker='o', color='w', label='TfidfVectorizer with Grid Search Learner SGD',\n",
    "                          markerfacecolor='g', markersize=8),\n",
    "     Line2D([0], [0], marker='o', color='w', label='TfidfVectorizer with Single Learner Passive',\n",
    "                          markerfacecolor='r', markersize=8)\n",
    "]\n",
    "fig, ax = plt.subplots()\n",
    "ax.legend(handles=legend_elements, loc = 'center')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = 0\n",
    "for val in df.iloc[:,1]:\n",
    "    print(o, val)\n",
    "    print()\n",
    "    o+=1\n",
    "\n",
    "st = [\"TfidfVectorizer with Single Learner MultiNB\", \n",
    "\"TfidfVectorizer with Single Learner MultiNB\", \n",
    "      \"TfidfVectorizer with Single Learner MultiNB\", \n",
    "       \"TfidfVectorizer with Single Learner MultiNB\", \n",
    "       \"CountVectorizer with Single Learner MultiNB\", \n",
    "       \"CountVectorizer with Single Learner MultiNB\", \n",
    "       \"TfidfVectorizer with Bagged Learner Passive\", \n",
    "\"Tfidf with Single Learner LinSVC\",\n",
    " \"Tfidf and Grid Search Learner SGD\", \n",
    " \"Tfidf and Bagged Learner Passive C: 0.1\",\n",
    " \"Tfidf and Single Learner Passive C: 0.01\", \n",
    " \"Tfidf and Single Learner Passive C: 1 stop_word list: default\", \n",
    " \"Tfidf and Single Learner Passive C: 1 stop_word list: None\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"movie3.csv\", header = None)\n",
    "list = my_pareto(df2[df2.columns[2:4]].values)\n",
    "df2 = df2.iloc[list]\n",
    "df2 = df2.sort_values(by=[2])\n",
    "df2.iloc[24, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
