{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the necessary imports\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is an example of a movie review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./aclImdb/train/pos/10327_7.txt', encoding=\"utf8\")\n",
    "print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting the train and test data into a pandas dataframe. It shuffles the data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./aclImdb\" #Make sure you put the data folder in the same directory as this jupyter notebook file\n",
    "labeledData = {}\n",
    "for i in [\"train\", \"test\"]:\n",
    "    labeledData[i] = []\n",
    "    for sentiment in [\"pos\", \"neg\"]:\n",
    "        score = 1 if sentiment == \"pos\" else 0\n",
    "        path = os.path.join(directory, i, sentiment)\n",
    "        for filename in os.listdir(path):\n",
    "            with open(os.path.join(path, filename), encoding=\"utf8\") as f:\n",
    "                labeledData[i].append([f.read(), score])  #Initially adds them to separate lists\n",
    "\n",
    "np.random.shuffle(labeledData[\"train\"]) #Shuffling\n",
    "labeledData[\"train\"] = pd.DataFrame(labeledData[\"train\"], columns = ['text', 'sentiment']) #Putting them in a dataframe\n",
    "np.random.shuffle(labeledData[\"test\"])\n",
    "labeledData[\"test\"] = pd.DataFrame(labeledData[\"test\"], columns = ['text', 'sentiment'])\n",
    "labeledData[\"train\"], labeledData[\"test\"] #Prints out both pandas dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = labeledData[\"train\"].to_csv(r'C:\\Users\\mdodda3-gtri\\Untitled Folder\\train.csv.gz', index = None, header=False, compression='gzip')\n",
    "test = labeledData[\"test\"].to_csv(r'C:\\Users\\mdodda3-gtri\\Untitled Folder\\test.csv.gz', index = None, header=False, compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train1 = labeledData[\"train\"].iloc[:6250].to_csv (r'C:\\Users\\mdodda3-gtri\\Untitled Folder\\train1.csv.gz', index = None, header=False, compression='gzip')\n",
    "# train2 = labeledData[\"train\"].iloc[6250:12500].to_csv (r'C:\\Users\\mdodda3-gtri\\Untitled Folder\\train2.csv.gz', index = None, header=False, compression='gzip')\n",
    "# train3 = labeledData[\"train\"].iloc[12500:18750].to_csv (r'C:\\Users\\mdodda3-gtri\\Untitled Folder\\train3.csv.gz', index = None, header=False, compression='gzip')\n",
    "# train4 = labeledData[\"train\"].iloc[18750:].to_csv (r'C:\\Users\\mdodda3-gtri\\Untitled Folder\\train4.csv.gz', index = None, header=False, compression='gzip')\n",
    "\n",
    "# test1 = labeledData[\"test\"].iloc[:6250].to_csv (r'C:\\Users\\mdodda3-gtri\\Untitled Folder\\test1.csv.gz', index = None, header=False, compression='gzip')\n",
    "# test2 = labeledData[\"test\"].iloc[6250:12500].to_csv (r'C:\\Users\\mdodda3-gtri\\Untitled Folder\\test2.csv.gz', index = None, header=False, compression='gzip')\n",
    "# test3 = labeledData[\"test\"].iloc[12500:18750].to_csv (r'C:\\Users\\mdodda3-gtri\\Untitled Folder\\test3.csv.gz', index = None, header=False, compression='gzip')\n",
    "# test4 = labeledData[\"test\"].iloc[18750:].to_csv (r'C:\\Users\\mdodda3-gtri\\Untitled Folder\\test4.csv.gz', index = None, header=False, compression='gzip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column contains the movie reviews in separated rows.\n",
    "The second column indicates whether the review is a positive or negative review. \n",
    "A positive reivew has 7-10 stars, A negative review has 1-4 stars. 5-6 stars are disregarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total = np.vstack((labeledData[\"train\"][\"text\"], labeledData[\"test\"][\"text\"]))\n",
    "#total = np.array([labeledData[\"test\"][\"text\"]])\n",
    "for col in total:\n",
    "    print(col.shape)\n",
    "total.reshape(1,total.shape[1] * total.shape[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pd.DataFrame(labeledData[\"train\"][\"text\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import  word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "stemmer =SnowballStemmer(\"english\")\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the movie reviews into Specific Bag of Words Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn imports\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit, LogisticRegression, SGDClassifier, PassiveAggressiveClassifier, Perceptron, RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, ComplementNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.feature_extraction import stop_words as stopwords\n",
    "\n",
    "\n",
    "\n",
    "#Stop word list created to be used\n",
    "stop_words = ['in', 'of', 'at', 'a', 'the']\n",
    "\n",
    "#how each review will be vectorized\n",
    "vectorizer = CountVectorizer(stop_words=stop_words, #These stop words are removed\n",
    "                             binary=True, # if it contains in list it's 1, else it is 0\n",
    "                             ngram_range=(1,2), #contains pairs of words as well\n",
    "                            \n",
    "                            )\n",
    "newVect = CountVectorizer (tokenizer = LemmaTokenizer(),\n",
    "                           stop_words=stop_words,\n",
    "                           binary=True,\n",
    "                           ngram_range=(1,2))\n",
    "\n",
    "\n",
    "\n",
    "x_train = vectorizer.fit_transform(labeledData[\"train\"][\"text\"]) #reviews from train dataframe are vectorized\n",
    "x_test = vectorizer.transform(labeledData[\"test\"][\"text\"])  #reviews from test dataframe are vectorized\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer, one_hot, text_to_word_sequence\n",
    "import keras\n",
    "import gensim\n",
    "train_data = labeledData[\"train\"][\"text\"]\n",
    "test_data = labeledData[\"test\"][\"text\"]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "#x_train = tokenizer.texts_to_sequences(train_data)\n",
    "#x_test = tokenizer.texts_to_sequences(test_data)\n",
    "type(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train\n",
    "#x_train = np.array([np.array(xi) for xi in x_train])\n",
    "#x_test = np.array([np.array(xi) for xi in x_test])\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xencoded = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=200, padding='post')\n",
    "# XencodedTest = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=200, padding = 'post')\n",
    "\n",
    "# model = keras.models.Sequential()\n",
    "# embedding = keras.layers.embeddings.Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=300, input_length=200, trainable=True, mask_zero=True)\n",
    "# model.add(embedding)\n",
    "# model.add(keras.layers.LSTM(units=150, dropout=0.2, recurrent_dropout=0.2, return_sequences=False))\n",
    "# model.add(keras.layers.Dense(2, activation='softmax'))\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "# print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# ss = StratifiedShuffleSplit(n_splits = 1, test_size= 0.2, random_state=1).split(Xencoded, labeledData[\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels = keras.utils.to_categorical(labeledData[\"train\"][\"sentiment\"], 2)\n",
    "# test_labels = keras.utils.to_categorical(labeledData[\"test\"][\"sentiment\"], 2)\n",
    "# early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=2, mode='auto', restore_best_weights=False)\n",
    "# history = model.fit(x=x_train, y=train_labels, epochs=50, batch_size=32, shuffle=True, validation_data = (XencodedTest, test_labels), verbose=2, callbacks=[early_stop])\n",
    "# predicted = model.predict(test_x, verbose=2)\n",
    "# predicted_labels = predicted.argmax(axis=1)\n",
    "\n",
    "l = [\"34\", \"34\"]\n",
    "l=[\"d\"]\n",
    "l = ''.join(l)\n",
    "\n",
    "a = np.array([l], dtype = object)\n",
    "b = np.array([\"ldkjsf\"], dtype = object)\n",
    "np.vstack([a,b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model, training, and applying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Any linear model can be used\n",
    "model = LinearSVC(C=1)  \n",
    "#This is another model that can be used\n",
    "model2 = LogisticRegression()   \n",
    "model4 = SGDClassifier()\n",
    "model5 = PassiveAggressiveClassifier()\n",
    "model6 = NuSVC()\n",
    "model6 = NearestCentroid()\n",
    "#model trained associating review vectors to it sentiment scores \n",
    "model.fit(x_train, labeledData[\"train\"][\"sentiment\"]) \n",
    "#applying model on test data creates what model thinks is sentiment scores associated with each movie review\n",
    "y_pred = model.predict(x_test) \n",
    "#accuracy score created by comparing to actual sentiment score to model's predicted sentiment score\n",
    "acc = accuracy_score(labeledData[\"test\"][\"sentiment\"], y_pred)\n",
    "print(\"Accuracy score of model: \"+str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix as cm\n",
    "cm(labeledData[\"test\"][\"sentiment\"], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#This is a the representation of the matrix of all the vectorized movie reviews. \n",
    "#The data is very sparse, so this data only shows the position on if a word is contained in a movie review\n",
    "#The first number in the ordered pair represents the movie review. \n",
    "#The second number in the ordered pair represents a particular word.\n",
    "#The row of 1 shows that it is the word is there in the movie review.\n",
    "#Everthing else are 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.hstack((x_train[0], np.array(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This prints out the first 10 n-grams associated with the first review above and the correspoding indeces for each n-gram\n",
    "count = 10;\n",
    "for i in x_train[0].nonzero()[1]:\n",
    "    print(str(i)+\": \" + vectorizer.get_feature_names()[i])\n",
    "    count-=1\n",
    "    if (count == 0):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "#gzip.open(\"C:\\Users\\mdodda3-gtri\\emade\\datasets\\movie_reviews\\GoogleNews-vectors-negative300.bin.gz\")\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pipeline to make it easier (puts, vectorizer and model in one line)\n",
    "Using Tfidf by transforming original countvectorizer using Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(labeledData[\"train\"][\"text\"])\n",
    "# x_train = tokenizer.texts_to_sequences(labeledData[\"train\"][\"text\"])\n",
    "# x_test =  tokenizer.texts_to_sequences(labeledData[\"train\"][\"text\"])\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stop_words = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "stop_words = ['in', 'of', 'at', 'a', 'the']\n",
    "def tokenize(val):\n",
    "    ans = text_to_word_sequence(val)\n",
    "    ans = [a for a in ans if not a in stop_words]\n",
    "    return ans\n",
    "reviews = labeledData[\"train\"][\"text\"].append(labeledData[\"test\"][\"text\"], ignore_index=True).values\n",
    "\n",
    "words = [tokenize(val) for val in reviews.tolist()]\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences=words, size=100, window = 1000, workers = 4, min_count=1, sg = 0)\n",
    "\n",
    "def method(list, wv):\n",
    "    mean = []\n",
    "    for word in list:\n",
    "        if word in wv.vocab:\n",
    "            mean.append(wv[word])\n",
    "        else:\n",
    "            a.append(word)\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = np.array([method(review, wv) for review in reviews.tolist()])\n",
    "#x_train.shape\n",
    "x_train = words[:25000]\n",
    "x_train = np.array([method(review, model.wv) for review in x_train])\n",
    "x_test = words[25000:]\n",
    "x_test = np.array([method(review, model.wv) for review in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.85428\n",
    "#159.6575005054474 seconds\n",
    "## CBOW, size changing\n",
    "#10: .75536\n",
    "#25: \n",
    "#50: .84576\n",
    "#100:.8596 ~40\n",
    "#500: .87052 ~76\n",
    "#750: .87104\n",
    "#1000: .87128 ~116.53\n",
    "\n",
    "##CBOW window\n",
    "#1 .833\n",
    "#5 .8501\n",
    "#10 .86 ~45\n",
    "#25 .87 ~ 57\n",
    "#50 same as 25\n",
    "#100 .89 105\n",
    "#200 .887 140\n",
    "#500 .892\n",
    "#1000 .894\n",
    "\n",
    "##Skip-Gram\n",
    "#25: .85\n",
    "#50: .87464\n",
    "#100: .88 ~ 154\n",
    "#200: .89 ~ 190\n",
    "#500:  .89 322   ~ (300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()\n",
    "\n",
    "model.fit(x_train, labeledData[\"train\"][\"sentiment\"]) \n",
    "#applying model on test data creates what model thinks is sentiment scores associated with each movie review\n",
    "y_pred = model.predict(x_test) \n",
    "#accuracy score created by comparing to actual sentiment score to model's predicted sentiment score\n",
    "acc = accuracy_score(labeledData[\"test\"][\"sentiment\"], y_pred)\n",
    "print(\"Accuracy score of model: \"+str(acc))\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index\n",
    "#wv.syn0norm\n",
    "#wv.wv.most_similar(\"little\")\n",
    "# for word, i in tokenizer.items():\n",
    "wv.vocab[\"hello\"].count\n",
    "wv[\"hello\"]\n",
    "tokenizer.word_index\n",
    "\n",
    "def tokenize(val):\n",
    "    ans = text_to_word_sequence(val)\n",
    "    ans = [a for a in ans if not a in stop_words]\n",
    "    return ans\n",
    "\n",
    "x_train_list = [tokenize(val) for val in labeledData[\"train\"][\"text\"].values.tolist()]\n",
    "x_test_list = [tokenize(val) for val in labeledData[\"test\"][\"text\"].values.tolist()]\n",
    "a =[]\n",
    "def method(list, wv):\n",
    "    mean = []\n",
    "    for word in list:\n",
    "        if word in wv.vocab:\n",
    "            mean.append(wv[word])\n",
    "        else:\n",
    "            a.append(word)\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "x_train = np.array([method(review, wv) for review in x_train_list])\n",
    "x_test = np.array([method(review, wv) for review in x_test_list])\n",
    "\n",
    "\n",
    "model = LinearSVC()\n",
    "\n",
    "model.fit(x_train, labeledData[\"train\"][\"sentiment\"]) \n",
    "#applying model on test data creates what model thinks is sentiment scores associated with each movie review\n",
    "y_pred = model.predict(x_test) \n",
    "#accuracy score created by comparing to actual sentiment score to model's predicted sentiment score\n",
    "acc = accuracy_score(labeledData[\"test\"][\"sentiment\"], y_pred)\n",
    "print(\"Accuracy score of model: \"+str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list[0]\n",
    "print(a[89])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # nltk.download('punkt')\n",
    "# # [nltk.word_tokenize(sentences) for sentences in train]\n",
    "# # model = gensim.models.Word2Vec(sentences = )\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# wv.wv.most_similar('horrible')\n",
    "\n",
    "\n",
    "\n",
    "# tokenizer_obj = Tokenizer()\n",
    "# tokenizer_obj.fit_on_texts(labeledData[\"train\"][\"text\"])\n",
    "# sequences = tokenizer_obj.texts_to_sequences(labeledData[\"train\"][\"text\"])\n",
    "# word_index = tokenizer_obj.word_index\n",
    "# max_length = max([len(s.split()) for s in labeledData[\"train\"][\"text\"] + labeledData[\"test\"][\"text\"]])\n",
    "# review_pad = pad_sequences(sequences, maxlen = max_length)\n",
    "\n",
    "# num_words = len(word_index) + 1\n",
    "# embedding_matrix = np.zeros((num_words, 100))\n",
    "\n",
    "# for word, i in word_index.items():\n",
    "#     if i > num_words:\n",
    "#         continue\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "# type(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier, AdaBoostClassifier,ExtraTreesClassifier\n",
    "\n",
    "\n",
    "#pipe = Pipeline([('vect', CountVectorizer(binary=True, ngram_range=(1,2))),('clf',LogisticRegression(C=.05))])fg\n",
    "stop_words = ['in', 'of', 'at', 'a', 'the']\n",
    "\n",
    "##pipe = Pipeline([('vect', CountVectorizer(binary=True, ngram_range=(1,2), stop_words=stop_words)), ('clf',LinearSVC(C=.01))])##\n",
    "\n",
    "#Making pipeline. has vectorizer, uses tfidf, and uses multinomialnb for the model\n",
    "pipe1 = Pipeline([('vect', CountVectorizer(tokenizer = LemmaTokenizer(), binary=True, ngram_range=(1,2), stop_words=stop_words)), \n",
    "                 ('clf',SGDClassifier())])\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(stop_words=stop_words, binary=True, ngram_range=(1,2) )), ('clf', SGDClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fits whole pipeline using the train text as x value and train sentiment as y value and makes model\n",
    "for pipe in [pipe1, pipe2]:\n",
    "    pipe.fit(labeledData[\"train\"][\"text\"], labeledData[\"train\"][\"sentiment\"]) \n",
    "    #model predicts sentiment scores using test dataframe\n",
    "    y_pred = pipe.predict(labeledData[\"test\"][\"text\"])\n",
    "    #compares model sentiment scores to actual sentiment scores\n",
    "    acc = accuracy_score(labeledData[\"test\"][\"sentiment\"], y_pred)\n",
    "    print(\"Accuracy score of the model: \"+ str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "# wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n =np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
